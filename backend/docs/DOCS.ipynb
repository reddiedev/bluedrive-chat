{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Planning\n",
    "\n",
    "When building full-stack applications, I begin by mapping out a comprehensive overview of each feature to be developed. This holistic, forward-thinking approach helps me anticipate challenges and avoid costly refactoring or rewrites that can result from incomplete planning—a lesson learned from past experience.\n",
    "\n",
    "For this project, I closely followed the provided outline and specifications, shaping my plan around the following core goals:\n",
    "- use the right tool for the job\n",
    "- keep solutions as simple as possible\n",
    "- prioritize developer experience, maintainability, and scalability\n",
    "\n",
    "### Main Phases\n",
    "1. LLM & Ollama Setup\n",
    "2. Database Setup\n",
    "3. Backend Setup\n",
    "4. Frontend Setup\n",
    "5. Testing and Deployment\n",
    "\n",
    "### Integration with Cursor\n",
    "\n",
    "To accelerate development and streamline problem-solving, I leveraged various AI agents through [Cursor](https://www.cursor.com/) and [ChatGPT/Claude](https://t3.chat/). Whenever I encountered challenges—such as refactoring, setting up boilerplate, or making architectural decisions—I consulted these tools. This approach not only helped me overcome obstacles efficiently but also enhanced my skills as a developer, as I used AI as a collaborative assistant rather than a crutch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment via Docker\n",
    "\n",
    "Right from the start, I planned to use Docker because it makes running and sharing apps much easier. I already had some experience with Docker, but I knew that working with several services at once and setting up their network connections could be tricky. Before this project, I hadn’t set up a Docker network in so much detail.\n",
    "\n",
    "To make things easier, I gave each service a fixed IP address and wrote these into the `docker-compose.yml` file. Doing this early on made it much simpler to connect everything together later. When it was time to link the services, it all worked smoothly.\n",
    "\n",
    "I also chose to use `docker compose` instead of running each container with `docker run` commands. I find `docker compose` much easier to work with, especially when you need to set up different settings or restart services often.\n",
    "\n",
    "Setting clear semantic names for the different services also helped when referencing them in commands in Docker.\n",
    "\n",
    "Overall, planning ahead with Docker saved me time and made the whole process go more smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base LLM: Gemma 3\n",
    "\n",
    "At first, it wasn’t clear what the main use of the chat app would be, so I didn’t have a specific goal when picking the language model. Because of this, I chose a model that is good at general knowledge, reasoning, and coding. I also wanted a model that is fast and doesn’t take up too much space, since this makes development and testing easier. I avoided models that focus only on reasoning, since they can be slower to start, and speed was more important for this project.\n",
    "\n",
    "To help me decide, I checked benchmark leaderboards like [WebDevArena](https://web.lmarena.ai/leaderboard) and [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#). After looking at the options, I picked `gemma3:1b`, which is a 1-billion parameter model. It’s new, performs well, and is small enough to run easily on most computers. I also like Google’s `Gemini 2.5 Pro` model, so I thought its \"open weights\" version would be a good fit too.\n",
    "\n",
    "You can read more about Gemma 3 in this [blog post](https://blog.google/technology/developers/gemma-3/).\n",
    "\n",
    "I also decided to add another model to compare `gemma3` with, `qwen3`, I chose it as one of the smallest new reasoning models, to see how it compares with gemma3. Since I like using other models like `qwen2.5-coder` from qwen, I found it interesting to compare the 2, especially the lower parameter count in qwen3 and would the reasoning capability compensate for that. Additionally, this provided a way for me to add model switching in the app so that comparing models can be easy.\n",
    "\n",
    "# LLM Provider: Ollama\n",
    "\n",
    "For running the models, I chose Ollama right away. I already use Ollama for my own projects, so I know how it works and how to set it up. It’s also popular in the developer community, so there are lots of guides and help available on places like Stack Overflow and GitHub. Ollama works on many operating systems and is easy to run with Docker.\n",
    "\n",
    "I followed this guide to set up Ollama with Docker: [Ollama Official Docker Image](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image).\n",
    "\n",
    "Since I already had Ollama running on my computer, I made sure to save the `.ollama` data in a separate volume and used a different port for this project. This way, it wouldn’t interfere with my other work.\n",
    "\n",
    "Additionally, I added the `OLLAMA_KEEP_ALIVE=24h` key in the compose setup in order to prevent model unloading when they are stale or have not been used recently. I found that this was good when testing different models and at different times - fortunately my GPU had enough VRAM to store all models that were tested at the same time\n",
    "\n",
    "Furthermore, I configured the `ollama/entrypoint.sh` script so that it downloads all of the models at startup, so that the setup is hands-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres Database\n",
    "\n",
    "The project instructions said to use Postgres, so I used this SQL database to handle the app’s data. Even if it wasn’t required, I would have picked Postgres anyway because I’m familiar with it. Postgres is popular, has lots of resources and plugins, and is widely used in real-world projects.\n",
    "\n",
    "To connect my app to Postgres, I used the [langchain-postgres](https://python.langchain.com/docs/integrations/memory/postgres_chat_message_history/) library. This library made it easy to set up the database tables for storing chat message history.\n",
    "\n",
    "However, `langchain-postgres` only saves the chat messages. I wanted more control, so I added my own table to keep track of different chat sessions. This lets me store things like session titles and link them to each user by their username. Later, I also added some sample data using a `database/seed.sql` file, so the table wouldn’t be empty during initial testing.\n",
    "\n",
    "Here’s the code I used to create the sessions table, it just used `psycopg` to write the raw SQL:\n",
    "\n",
    "```python\n",
    "def create_db_sessions_table(conn: Connection):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS db_sessions (\n",
    "                id UUID PRIMARY KEY,\n",
    "                username VARCHAR(255) NOT NULL,\n",
    "                title VARCHAR(255) NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "            \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgres Deployment via Docker\n",
    "\n",
    "Setting up Postgres with Docker can be a bit tricky because you need to be clear about things like the database username and password. Just like with Ollama, I made sure to use a unique port and a separate data volume for this project. This way, it wouldn’t interfere with any other Postgres instances I have running on my computer.\n",
    "\n",
    "I followed this guide for setting up Postgres with Docker: [How to use the Postgres Docker Official Image](https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/).\n",
    "\n",
    "Here’s the part of my `docker-compose.yml` file that sets up the database:\n",
    "\n",
    "```yaml\n",
    "database:\n",
    "  image: postgres:16\n",
    "  restart: always\n",
    "  container_name: bd_database\n",
    "  ports:\n",
    "    - \"${POSTGRES_PORT:-5432}:5432\"\n",
    "  environment:\n",
    "    POSTGRES_USER: ${POSTGRES_USER}\n",
    "    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n",
    "    POSTGRES_DB: ${POSTGRES_DB}\n",
    "  networks:\n",
    "    bd_network:\n",
    "      ipv4_address: \"172.28.0.30\"\n",
    "  volumes:\n",
    "    - bd_pgdata:/var/lib/postgresql/data\n",
    "    - ./database/seed.sql:/docker-entrypoint-initdb.d/seed.sql\n",
    "```\n",
    "\n",
    "> Please note that I injected the seeding script directly on the docker container, so that it would be initialized on startup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend Setup\n",
    "\n",
    "For the backend of this app, Python was the obvious choice because it works well with the tools and libraries needed for the project. I use Python a lot, so I felt comfortable building both small and large parts of the backend with it. For this project, I kept things simple and only used the modules and functions I really needed, while still following good coding practices.\n",
    "\n",
    "Instead of using pip to manage Python packages, I chose [uv](https://docs.astral.sh/uv/). I like uv because it’s fast (it’s built with Rust) and makes it easy to manage the Python environment and keep all the dependencies in sync. It feels a lot like using `pnpm` for JavaScript, but for Python projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Backend\n",
    "\n",
    "When working with LLMs, I have a lot of experience using [OpenAI API Spec](https://openai.com/index/openai-api/) services since they have a lot of SDKs as well as resources on how to use them. Honestly, I haven't used langchain prior to this project but after doing some research as well as hands-on experience, I appreciate the hands-free orchestration and simplified setup it offers as well as the community libraries that make bootstrapping applications much quicker and easier\n",
    "\n",
    "For this, I had three main problems I needed to achieve\n",
    "\n",
    "### Langchain Integration with Ollama\n",
    "Fortunately, there is already a resource that does this, [langchain-ollama](https://python.langchain.com/docs/integrations/llms/ollama/). Thus, I just used the provided boilerplate when interfacing Langchain with Ollama\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "```\n",
    "\n",
    "From there, setting up the system prompts, as well as using `langchain-postgres` to differentiate between the `HumanMessage` as well as `AIMessage`, setting up the chat workflows became easy.\n",
    "\n",
    "### Streaming Chat Completions\n",
    "\n",
    "Streaming responses from Ollama via Langchain was one of my first hurdles in the project, since I haven't used these before and thus I did not know how to setup streaming. I understood streaming from a network point of view but not how to do it via Python. Unfortunately, this was one of the areas as well that using ChatGPT faltered since it did not set it up correctly, and there were a lot of conflicting ways to do it via different langchain libraries that evolved over the years. Fortunately, I was able to find this github issue that started way back in 2023 that taught me how to implement streaming https://github.com/langchain-ai/langchain/issues/13333\n",
    "\n",
    "```python\n",
    "    model_with_streaming = OllamaLLM(\n",
    "        model=request.model,\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "\n",
    "    async def stream_response():\n",
    "        nonlocal full_response\n",
    "        async for token in model_with_streaming.astream(messages):\n",
    "            full_response += token\n",
    "            yield token\n",
    "\n",
    "```\n",
    "Turns out, you needed to define it explicitly in the `OllamaLLM` class as well as use the `astream` method. For a minute there I was stuck trying to implement the old methods like using `AsyncIteratorCallbackHandler` or `StreamingStdOutCallbackHandler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI Backend\n",
    "\n",
    "For managing the backend, I decided to expose all of these features via a REST HTTP server through [FastAPI](https://fastapi.tiangolo.com/). I like it because setting up a web server through it is very easy, with a lot of references and all the batteries already included\n",
    "\n",
    "I only had to setup the protocol for which the frontend later will interact with the backend for the different app operatinos\n",
    "\n",
    "```python\n",
    "class ChatRequest(BaseModel):\n",
    "    name: str = \"User\"\n",
    "    session_id: str\n",
    "    content: str\n",
    "    model: str = \"gemma3:1b\"\n",
    "\n",
    "@app.post(\"/stream\")\n",
    "async def chat(request: ChatRequest):\n",
    "    ## input validation & session handling\n",
    "\n",
    "    ## chat completion via langchain\n",
    "    chat_history = PostgresChatMessageHistory(\n",
    "        table_name, request.session_id, sync_connection=sync_connection\n",
    "    )\n",
    "\n",
    "    new_usr_msg = HumanMessage(\n",
    "        content=request.content, id=generate_message_id(), name=request.name\n",
    "    )\n",
    "    \n",
    "    model_with_streaming = OllamaLLM(\n",
    "        model=request.model,\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    # RESPONSE STREAMING\n",
    "    full_response = \"\"\n",
    "\n",
    "    async def stream_response():\n",
    "        nonlocal full_response\n",
    "        async for token in model_with_streaming.astream(messages):\n",
    "            full_response += token\n",
    "            yield token\n",
    "\n",
    "    ## streaming response\n",
    "    return StreamingResponse(\n",
    "        stream_response(), media_type=\"text/plain; charset=utf-8\"\n",
    "    )\n",
    "\n",
    "```\n",
    "Fortunately, FastAPI has a built-in helper for the streamed response using `StreamingResponse`. This made the setup much easier to work with\n",
    "\n",
    "### Background Task for Database Operation post-Streaming\n",
    "In order to sync the message state with the database while also returning the streamed chat completions as soon as possible to the client, I had to find a way to store the full response after streaming it, which was done via a `BackgroundTask`\n",
    "```python\n",
    "async def store_messages():\n",
    "    new_ai_msg = AIMessage(\n",
    "        content=full_response, id=generate_message_id(), name=\"Assistant\"\n",
    "    )\n",
    "    chat_history.add_messages([new_usr_msg, new_ai_msg])\n",
    "\n",
    "background_tasks.add_task(store_messages)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the API via Insomnia\n",
    "\n",
    "Before building the frontend, I used [Insomnia](https://insomnia.rest/) to manually test the FastAPI endpoints and the Ollama integration. This allowed me to quickly verify that the backend was working as expected and that the chat completions were being streamed correctly.\n",
    "\n",
    "Here are some examples which I exported as `curl` commands:\n",
    "\n",
    "Getting the available models in Ollama to make sure that they were loaded correctly\n",
    "```bash\n",
    "curl --request GET \\\n",
    "  --url 'http://localhost:11434/api/tags?name=John%20Doe' \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --header 'User-Agent: insomnia/11.2.0'\n",
    "```\n",
    "\n",
    "```bash\n",
    "curl --request POST \\\n",
    "  --url http://localhost:8000/stream \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --header 'User-Agent: insomnia/11.2.0' \\\n",
    "  --data '{\n",
    "\t\"name\": \"Red\",\n",
    "\t\"session_id\": \"b95eecaa-f30f-45b1-bcc2-acb984dce9a5\",\n",
    "\t\"content\": \"Who are you?\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# React Frontend\n",
    "\n",
    "Instead of using the suggested Python UI libraries, I chose to build the frontend with React and TypeScript. I made this choice because I’m already comfortable with React, so it was much faster for me to get started and connect it to the Python backend. Using React also gave me more control over how the app looks and works, letting me create a user interface that is both good-looking and easy to use.\n",
    "\n",
    "Here are the main libraries and tools I used:\n",
    "- [React](https://react.dev/) – for building the user interface\n",
    "- [Vite](https://vite.dev/) – for fast bundling and serving of React code (faster than Next.js or Create React App)\n",
    "- [Tanstack Start](https://tanstack.com/start/latest) – for easy file routing and server functions using RPCs\n",
    "- [TailwindCSS](https://tailwindcss.com/) – for quick and simple styling with utility classes\n",
    "- [shadcn/ui](https://ui.shadcn.com/) – for ready-to-use, reusable UI components\n",
    "\n",
    "## User Handling\n",
    "I used [react-hook-form](https://www.react-hook-form.com/) with [zod](https://zod.dev/) in order to validate the input for the user's username. This made it easy for me since the boilerplate was already setup, and I just passed the user's username as a URL parameteter to the chat page\n",
    "\n",
    "```tsx\n",
    "const form = useForm<z.infer<typeof usernameFormSchema>>({\n",
    "  resolver: zodResolver(usernameFormSchema),\n",
    "  defaultValues: {\n",
    "    username: \"\",\n",
    "  },\n",
    "})\n",
    "\n",
    "function onSubmit(values: z.infer<typeof usernameFormSchema>) {\n",
    "  // generate uuid v4 \n",
    "  const { username } = values\n",
    "  const session_id = crypto.randomUUID()\n",
    "  navigate({ to: '/chat/$session_id', params: { session_id }, search: { username } })\n",
    "}\n",
    "```\n",
    "## Threads Sidebar\n",
    "First, there is a navigation component that basically retrieves a user's list of past threads, if any, so that the user can read or chat with any of them again. One of the problems I had with this one was providing a way to render the title with the emoji correctly, truncate long titles, as well as have a snappy feedback for whenever a user \"creates\" a new thread \n",
    "\n",
    "You can see here the code that gets the sessions from the `/sessions` endpoint in the Backend API\n",
    "\n",
    "```ts\n",
    "export const getSessions = createServerFn({\n",
    "  method: 'GET',\n",
    "  response: 'data',\n",
    "}).validator(({ name, session_id }: { name: string, session_id: string }) => {\n",
    "  return {\n",
    "    name: name,\n",
    "    session_id: session_id,\n",
    "  }\n",
    "}).handler(async ({ data }) => {\n",
    "  try {\n",
    "    const url = `${import.meta.env.VITE_BACKEND_BASE_URL}/sessions?name=${data.name}`\n",
    "  \n",
    "    const response: AxiosResponse<SessionData[]> = await axios.get(url)\n",
    "\n",
    "    const sessions = response.data.slice(0, 15)\n",
    "\n",
    "    // check if session_id is in the sessions, if not, add it at the top of the list with the title \"New Thread\"\n",
    "    if (!sessions.some((session) => session.id === data.session_id)) {\n",
    "      sessions.unshift({\n",
    "        id: data.session_id,\n",
    "        title: \"🧵 New Thread\",\n",
    "        username: data.name,\n",
    "        isNew: true,\n",
    "      })\n",
    "    }\n",
    "\n",
    "    return sessions\n",
    "  } catch (err) {\n",
    "    // console.error(err)\n",
    "    return []\n",
    "  }\n",
    "})\n",
    "```\n",
    "I found that setting up all of the validation, as well as the static types for the returned JSON was good since it is easier to read and debug, can be understood by AI agents, as well as the autocomplete is just a quality of life that is leagues ahead of other languages or frameworks.\n",
    "\n",
    "## Thread/Session Handling\n",
    "In order to simplify it, as well as seeing how other chat apps do it, each \"thread\" or \"conversation\" is represented by a unique UUID to reference that thread. Thus, I just render this via the `/chat/<uuid>` endpoint in the website. \n",
    "\n",
    "\n",
    "Thus, the UI just needs to do three things:\n",
    "- retrieve old messages (for old threads)\n",
    "- provide a way to input a user's message\n",
    "- provide a way to get the assistant's response\n",
    "\n",
    "## Message Input\n",
    "This one took me a while to setup, as I had to setup a `<Select/>` component which allows the user to select what chat model to use (from the available models via the Backend API). I also had to setup a `<Textarea/>` component that allows the user to input multi-line text for the Chat workflow.\n",
    "\n",
    "I also invested some time making it pretty, and fixed to the chat window at the bottom. I like how the glassmorphism effect is rendered through the built-in `background opacity` & `backdrop blur` in Tailwind\n",
    "\n",
    "```tsx\n",
    "<div className=\"isolate backdrop-blur-sm flex flex-row items-center gap-2 p-2 bg-neutral-800/20 border-[1px] border-b-0 border-neutral-800 rounded-xl rounded-b-none pb-0\">\n",
    "  <div className=\"isolate rounded-xl bg-neutral-800/20 border-[1px] border-b-0 border-neutral-800 w-full flex flex-col rounded-b-none pb-0\">\n",
    "    <div className=\"p-2\">\n",
    "      <Textarea />\n",
    "    </div>\n",
    "    <div className=\"p-2 pt-0 flex justify-between items-center\">\n",
    "      <Select/>\n",
    "      <Button size=\"icon\" id='send-message-button' type=\"submit\" className='cursor-pointer'>\n",
    "        <ArrowUp />\n",
    "      </Button>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "## Chat Streaming\n",
    "Once the user input is received, the Frontend Client just sends an HTTP request to the Backend API and waits for the Streamed Response. This streamed response is streamed to the Client State so that it can be smoothly rendered in the UI - producing the \"typing\" animation on the interface.\n",
    "\n",
    "```tsx\n",
    "const response = await fetch(STREAMING_URL, {\n",
    "  method: 'POST',\n",
    "  headers: {\n",
    "    'Content-Type': 'application/json',\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    name: username,\n",
    "    session_id: session_id,\n",
    "    content: content,\n",
    "    model: values.model\n",
    "  })\n",
    "})\n",
    "\n",
    "if (!response.ok) {\n",
    "  const errorData = await response.json()\n",
    "  throw new Error(errorData.detail || 'Failed to send message')\n",
    "}\n",
    "\n",
    "const reader = response.body?.getReader()\n",
    "if (!reader) {\n",
    "  throw new Error('No reader available')\n",
    "}\n",
    "\n",
    "let accumulatedContent = ''\n",
    "\n",
    "while (true) {\n",
    "  const { done, value } = await reader.read()\n",
    "  if (done) break\n",
    "\n",
    "  // Convert the Uint8Array to text\n",
    "  const chunk = new TextDecoder().decode(value)\n",
    "  accumulatedContent += chunk\n",
    "\n",
    "  // Update the message with accumulated content\n",
    "  setMessages(prevMessages =>\n",
    "    prevMessages.map(msg =>\n",
    "      msg.id === tempMessageId\n",
    "        ? { ...msg, content: accumulatedContent }\n",
    "        : msg\n",
    "    )\n",
    "  )\n",
    "}\n",
    "```\n",
    "\n",
    "## Conversation Display\n",
    "I created a `<MessagesContainer/>` component that renders the messages from the backend (whether it be from user or assistant) and displays it through the UI\n",
    "```tsx\n",
    "function MessagesContainer({ messages }: { messages: MessageData[] }) {\n",
    "  const { username } = Route.useSearch()\n",
    "\n",
    "  return (\n",
    "    <div className='flex flex-col grow pb-40'>\n",
    "      {messages.map((message) => (\n",
    "        <MessageBox key={message.id} message={message} />\n",
    "      ))}\n",
    "\n",
    "    </div>\n",
    "  )\n",
    "}\n",
    "```\n",
    "\n",
    "This component only \"listens\" for new messages from the user or the assistant via the Backend API and renders it to the UI as soon and as smoothly as possible\n",
    "\n",
    "## QoL: Markdown Rendering and Syntax Highlighting\n",
    "Since the chat UI was probably going to be used for coding tasks, one of the features that I tried to implement was syntax highlighting. This was hard to implement in React since there is not a lot of resources on doing it easily and allowing suport for multiple languages.\n",
    "\n",
    "Fortunately, after a lot of trial and error, I was able to make [highlight.js](https://highlightjs.org/) work, which makes reading code snippets on the UI much easier on the eyes. I also found that explicitly stating to the LLM through the system prompt to follow best markdown practices by explicitly stating the language also helped\n",
    "\n",
    "```tsx\n",
    "<ReactMarkdown\n",
    "  remarkPlugins={[remarkGfm]}\n",
    "  rehypePlugins={[rehypeHighlight, rehypeRaw]}\n",
    "  components={{\n",
    "    // Custom styling for code blocks\n",
    "    pre: ({ children, ...props }) => (\n",
    "      <pre\n",
    "        {...props}\n",
    "        className=\"bg-gray-100 dark:bg-gray-800 rounded p-2 overflow-x-auto text-xs\"\n",
    "      >\n",
    "        {children}\n",
    "      </pre>\n",
    "    ),\n",
    "    // Custom styling for inline code\n",
    "    code: ({ children, className, ...props }: any) => {\n",
    "      const match = /language-(\\w+)/.exec(className || '')\n",
    "      return match ? (\n",
    "        <code\n",
    "          {...props}\n",
    "          className={cn(\n",
    "            'hljs',\n",
    "            className,\n",
    "            'bg-gray-100 dark:bg-gray-800 px-1 rounded text-xs  overflow-x-scroll scrollbar-thin'\n",
    "          )}\n",
    "        >\n",
    "          {children}\n",
    "        </code>\n",
    "      ) : (\n",
    "        <code\n",
    "          {...props}\n",
    "          className=\"bg-gray-100 dark:bg-gray-800 px-1 rounded text-xs overflow-x-scroll scrollbar-thin\"\n",
    "        >\n",
    "          {children}\n",
    "        </code>\n",
    "      )\n",
    "    },\n",
    "  }}\n",
    ">\n",
    "  {message.content}\n",
    "</ReactMarkdown>\n",
    "```\n",
    "\n",
    "## QoL: Auto-scroll\n",
    "Additionally, one of the much-needed features was the auto-scroll for the UI, which makes the animation of the typing sequence in the streamed response much smoother. I honestly didn't know how to do it, but using AI and testing helped me figure it out\n",
    "```tsx\n",
    "  useEffect(() => {\n",
    "    if (mainRef.current) {\n",
    "      mainRef.current.scrollTo({\n",
    "        top: mainRef.current.scrollHeight,\n",
    "        behavior: 'smooth'\n",
    "      })\n",
    "    }\n",
    "  }, [messages])\n",
    "```\n",
    "\n",
    "Note that the effect listens for the updates to the `messages` state, which allows me to scroll on each message update from the streamed response - this made the UI feel much more natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "This phase of the project was admittedly not very hard, but it's actually one of parts I enjoyed the most. Truly, I haven't done any large-scale testing in my apps before since it was not needed, so having the time to practice it was both a good learning experience and also a fun task to do.\n",
    "\n",
    "From research, I found that using [pytest](https://docs.pytest.org/en/stable/) was one of the industry standards. \n",
    "\n",
    "Some suggest that it would be good to make tests before doing the features, to have \"test-driven development\". For me, I believe that a mix of both is good. Since the tests are being created now, maintaining the project and adding more features in the future will be much easier.\n",
    "\n",
    "Funnily enough, I was actually able to identify a bug in the Backend API after implementing testing, wherein I found that the `/session` endpoint was not implementing proper input validation, and not outputting the correct HTTP error code. \n",
    "\n",
    "Below are some examples of the tests I had done \n",
    "\n",
    "```python\n",
    "def test_unknown_method():\n",
    "    response = client.put(\"/health\")\n",
    "    assert response.status_code == 405\n",
    "```\n",
    "This test was actually good, since although FastAPI already handled the boilerplate for me, it gave me a wider view of the project\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "@pytest.mark.selenium\n",
    "def test_continue_with_long_username(driver):\n",
    "    \"\"\"Test that submitting the form with a long username shows the correct error message.\"\"\"\n",
    "    test_username = \"testuser123\" * 10\n",
    "    expected_message = \"Username is too long\"\n",
    "\n",
    "    time.sleep(1)  # Wait for page to load\n",
    "\n",
    "    # Find and fill the username input\n",
    "    username_input = driver.find_element(\"id\", \"username-input\")\n",
    "    username_input.send_keys(test_username)\n",
    "\n",
    "    # Click the continue button\n",
    "    button = driver.find_element(\"id\", \"continue-button\")\n",
    "    button.click()\n",
    "\n",
    "    # Wait for the form message to appear (max 5 seconds)\n",
    "    message_element = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-slot=\"form-message\"]'))\n",
    "    )\n",
    "\n",
    "    # Assert the message matches the expected text\n",
    "    assert message_element.text == expected_message, (\n",
    "        f\"Expected message '{expected_message}' but got '{message_element.text}'\"\n",
    "    )\n",
    "```\n",
    "\n",
    "It was also fund doing an E2E test with the React frontend and ensuring all the form validation is correct by using [Selenium](https://www.selenium.dev/), the foremost web browser driver in Pythonland."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Deployment\n",
    "\n",
    "Since I had already planned for deploying via Docker from the start, deploying each service became an easy task afterward. Thus, I only needed to double check all the config and connections if they were correct, since each service was already containerized and decoupled from one another.\n",
    "\n",
    "\n",
    "### .env handling in Frontent\n",
    "Unfortunately, I had some problems doing in the .env management in the Frontend since Vite does it differently from what I was accustomed to. They had their own `import.meta.env` syntax for declaring and accessing the environment secrets, and these were statically replaced at build time - a feature I did not understand truthfully. I found that the environment variables were being undefined at different areas (client and server) and erratically depending on the method (`process.env` or `import.meta.env`). There was also the `VITE_` prefix that was necessary in order for it to be readable on the client.\n",
    "\n",
    "- .env handling in Vite: https://vite.dev/guide/env-and-mode\n",
    "- the Stackoverflow post that made me find out it was Docker build: https://stackoverflow.com/questions/77486735/docker-with-vite-env-variables-are-undefined-inside-the-docker-container\n",
    "\n",
    "Eventually, I found out that a weird quirk in the build process meant that the .env was not available to the builder in Docker. I only had to set it up as a build argument in the Dockerfile and it finally worked in the docker container.\n",
    "\n",
    "\n",
    "### Container-to-container and Host-Container Networking in Docker\n",
    "After deploying, I found that there were issues when running the entire stack in different environments and OS. I found that a weird quirk in the client-side (browser) code on the React app which was connecting to the Backend (through the Docker network) was failing because, depending on how the OS and Docker handles DNS, the IP address for the backend service was not accessible (because I also set static IPs in the `docker-compose.yml` file)\n",
    "\n",
    "After rigorous testing across different devices, I refactored my Frontend app to have logic within the server-side environment to not have the browser/client side interface directly with the Backend/Database/Ollama. Furthermore, I simplified the port and IP assignment in the compose file by setting correct dependencies, and letting docker handle the IP assignment and networking e.g. `http://bd_backend:8000` instead of explicitly assigning `http://172.28.0.20:8000` (which, on hindsight, would really NOT be accessible on client side)\n",
    "\n",
    "Furthermore, I found that exposing the services on the same ports on the containers as well as on host would reduce confusions on testing and development. For example, the backend was running on port `8002` on my host machine, which was forwarded to port `8000` of the container - it became a headache to put all of these assignments in your mental map. So I just simplified it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Working on this project was a great learning experience for me. I got to use a mix of technologies—like Docker, Postgres, Langchain, Ollama, and React—and learned how they can all work together to build a seamless containerized full-stack app. Setting up things like Docker networks and managing dependencies with new tools like `uv` helped me understand more about modern development workflows.\n",
    "\n",
    "I also had a lot of fun exploring new libraries and frameworks, and I enjoyed the challenge of connecting everything smoothly. Building both the backend and frontend gave me a better idea of how to design apps that are both powerful and easy to use.\n",
    "\n",
    "Overall, this project expanded my vision and skillset. I feel more confident now in using these tools for future projects, and I’m excited to keep learning and building even more complex applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
