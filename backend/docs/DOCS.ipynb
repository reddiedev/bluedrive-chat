{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Planning\n",
    "\n",
    "When making full-stack apps, I start with drawing the full-picture of each of the features that I am going to build, and getting my mental map of the things I need to build and plan for. This forward-looking approach is something I try to strive by always, because I've had many instances in the past wherein I need to refactor a large feature or rebuild everything from scratch because my planning was incomplete or wrong\n",
    "\n",
    "Thus, I used the project outline and specifications laid out in the assessment and tried to formulate the project according to the following main goals:\n",
    "- use the right tool for the job\n",
    "- simple is best\n",
    "- prioritize developer experience, maintainability, and scalability\n",
    "\n",
    "## Main Phases\n",
    "1. LLM & Ollama Setup\n",
    "2. Database Setup\n",
    "3. Backend Setup\n",
    "4. Frontend Setup\n",
    "5. Testing and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment via Docker\n",
    "From the start, I wanted to plan ahead for using docker, since I knew the containerization as well as portability would be beneficial for the assessments done. I have experience working with docker, but I know that working with large stacks with many services, as well as setting up network connectivity would be a problem. I honestly don't have much experience in it. \n",
    "\n",
    "Therefore, I decided to setup static IPs for each of the services and explicitly declare them in the `docker-compose.yml` file in order to have it all ready by the time for development. This turned out to be a good idea since setting up the connections by the end was a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base LLM\n",
    "It was not clear what the exact use case of the Chat app will be, so there was no target field to prioritize when picking the LLM base model. Thus, I decided to go for a mix of general knowledge, reasoning, and coding ability as the main factors that I will use for picking a model. Speed and size was also a consideration, since faster inferences and smaller download sizes is a good boon in the development as well as assessment later on.\n",
    "\n",
    "For this one, I looked at benchmark leaderboards in [WebDevArena](https://web.lmarena.ai/leaderboard) as well as [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#) in order to narrow my search. After careful consideration, I decided to use `gemma3:1b` - a 1B-parameter model due to its recent performance as well as it being relatively new. I also enjoy using Google's `Gemini 2.5 Pro` model and I believe its open-weights counterpart would also be a suitable model. \n",
    "\n",
    "Later on, I also added the ability to load multiple models into the app, so that they can be easily compared. As such, I added `llama3.2:1b` as well as `qwen2.5-coder:1.5b` as I believe they are comparable models within the same size that could be used to compare \n",
    "\n",
    "# LLM Provider\n",
    "For the provider, I immediately decided to use Ollama because it is the one I am already using in my personal setup and side projects and thus have a good feel on how to use and deploy it. I also want to highlight its prevalence in the community with a lot of resources and other developers in stackoverflow and github issues that will help me should I need it. It is also relatively easy to deploy across many OS platforms and is already well-documented to be deployed via Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# React Frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
